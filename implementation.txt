DATASET DESCRIPTION:
The dataset consists of a single table with 14 columns.The dataset has a set of details of the taxi routes in NewYork City of few previous years.For simplification, the model takes a sample of 1,00,000(One lakh) rows from the dataset in random order.Each row in the dataset has an information about the medallion number of the taxi driver,the hack license of the taxi driver, vendor id, the VTS, rate code, number of passengers, total trip time in seconds, total distance of the trip, pickup location,pickup time,dropoff location and dropoff time.
medallion	hack_license	vendor_id	rate_code	store_and_fwd_flag	pickup_datetime	dropoff_datetime	passenger_count	trip_time_in_secs	trip_distance	pickup_longitude	pickup_latitude	dropoff_longitude	dropoff_latitude

PROGRAMMING LANGUAGE USED:
Following are the pre-requisites for the implementation of the project:
a) Programming Language: The programming languange used for the implementation is python 2.7.
Python has gathered a lot of attention recently as a very good alternative of programming language for analysing the data . It has many features:
1)It is Open Source, which means it can be installed in the system without paying charges.
2)It has an Excellent online community where programmers share new ideas and implementations of problems.
3)It is the simplest programming language ever.It is easy and clean to use.
4)It can become a common language for data science and production of web based analytics products.

For the implementation of the taxi recommender model, the platform used will be eclipse.
	Eclipse Neon 4.6:the Eclipse Neon 4.6 is very popular as an IDE for development for both python and java.The open source community of Eclipse is also very large and efficient. At the present moment the community consists of over 150 projects which includes various aspects of development of softwares. To use python in an Eclipse IDE there are two pre-requisites:
	
1)PyDev Package framework:
  This framework requires a python interpreter with python 2.7 or above.It can be installed into eclipse neon 4.6 in the help->get new software tab. Then the build path for the module is configured. This is done by adding the python library path to the path environment variable. Lastly, the runtime environment of the module is to be set for the execution of a pydev module. This is done by build->run->runtime environments tab and setting there a new python interpreter.
The PyDev module is a python based module used to do high scientific calculations and implement complex algorithms. It has variety of features which incredible support to increase performance of the system.It supports the Anacondda33 library which has a huge set of tools like mathematical  tools,relational tools, comparison tools  etc.It provides libraries to use complex mathematical  functions and implement algorithms in the most optimisd way.It also supports multithreading for parallel processing.The Pydev module supports importing any python module from different packages and so has a lot of accessibility features.We run the interpreter to experiment easily with small amounts of Python code. As beginners, we can easily perform experiments with the Python Interpreter to help us learn the simple Python language features: as more experienced programmers, we can easily investigate more complicated language features and libraries, while writing programs without leaving Eclipse. We will take both roles as the quarter progresses, so it is important to know how to access the Python Interpreter in Eclipse.
Pydev Project supports python modules to write simple python scripts,django framework to support application development.
	
2)Python Interpreter: The python interpreter used for the implementation of the model is Anaconda2.7 library.Anaconda is ideal for interactive, hands-on learning. Data Science teams can rapidly build, collaborate and deploy solutions using an agile approach with Anaconda. All training curriculum is designed and taught by experts and delivered live in an interactive environment.It provides strong libraries such as pandas and numpy for handling,managing and analysing large amounts of data and perform operations on them in an efficient manner.
The model is implemented by using assistance of these libraries:

	a)Pandas: Python has long been great for data munging and preparation, but less so for data analysis and modeling. pandas helps fill this gap, enabling to carry out the entire data analysis workflow in Python without having to switch to a more domain specific language like R.

	b)Numpy: NumPy is the fundamental package for scientific computing with Python. It contains a powerful N-dimensional array object
sophisticated (broadcasting) functions,tools for integrating C/C++ and Fortran code,useful linear algebra, Fourier transform, and random number capabilities. Besides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional container of generic data. Arbitrary data-types can be defined. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases.

	c)Matplotlib: matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like Tkinter, wxPython, Qt, or GTK+. There is also a procedural "pylab" interface based on a state machine (like OpenGL), designed to closely resemble that of MATLAB, though its use is discouraged.SciPy makes use of matplotlib.


ALGORITHM USED:

A)Importing test data and removing unnecessary columns
 
The test dataset consisting of all the NewYork city taxi routes is very large.The memory space required for storing the total data is over 10GBs. So, Sampling is performed on the dataset and the model choses journey information of 1 lakh taxi trips randomly from the dataset.This reduces size of the dataset to 14Mb.The dataset consists of the medallion number,hack license,VTS and the list goes on to a total of 14 columns for each of the one lakh taxi trips across NewYork city. Some these columns does not give informations relevant for the implementation, while some are insufficient to be processed to get information.The model reduces the data into eight columns which are:
	medallion: to identify the driver who carried the passengers 
	pickup_datetime: the time and date of the start of the trip
	dropoff_datetime: the time and date of the end of the trip 
	trip_time_in_secs:the total duration of the trip 
	trip_distance: total distance travelled during the trip 
	pickup_longitude:the longitutde of the location of start of the trip 
	pickup_latitude:the latitude of the location of start of the trip 
	dropoff_longitude:the longitutde of the location of end of the trip
	dropoff_latitude:the latitutde of the location of start of the trip      

B)Changing the pickup and dropoff timestamps to minutes for calculation:

The test dataset consists of time and date for start and end of the trip in a format like:'date-month-year hour:minutes:seconds'. The model takes off the date and changes the time in a computable format. Also, it is worth noting that the seconds in the data can also be neglected.
the pickup_datetime and dropoff_datetime columns with the value: datetime=hours*60+mins for each trip information in the dataset.

C)Removing rows with null values
	
the model implements exhaustive search methods to eliminate null value rows in the dataset.So, for each row in dataset, the rows containing null values are eliminated by exhaustive searching in O(n) time complexity.

D)Apply K-means clustering to the dataset with varying number of clusters: 

The model partitions the dataset into clusters using the K means clustering algorithm for a varying number of clusters between 2 to 13.K-means is an unsupervised learning algorithm that helps in arranging the data into clusters. The procedure follows an efficient way to arrange a given data set in certain number of clusters which are previously fixed. The main idea is to define some centroids, one for each cluster. These centroids shoud be placed in an efficient way because of different location causes different result. So, the better choice is to place them as much as possible far away from each other. The next step is to take each point belonging to a given data set and associate it to the nearest centroid. When no point is pending, the first step is completed and an early groupage is done. At this point the model needs to re-calculate new centroids as barycenters of the clusters resulting from the previous step. After we have these new centroids, a new binding has to be done between the same data set points and the nearest new centroid. A loop has been generated. As a result of this loop the final centroids change their location step by step until no more changes are done. In other words centroids do not move any more.
Finally, the algorithm returns some previously defined number of centroids,one for each cluster that represent most similar values in the cluster. The algorithm also returns a label to each point in the dataset that indicate the cluster to which the point belongs.
The algorithm is implemented in the dataset using the following steps: 

	1)Dropping unnecessary columns(PCA): The current dataset is transformed into a reduced dataset for efficient calculations. The reduction is done in such a way that there is no or very negligible information loss. This is achieved by using PCA(Principle Component Analysis algorithm). Principal component analysis(PCA) is an algorithm,rather a mathematical procedure that transforms a number of possible inter-related attributes into a reduced number of attributes that are not inter related. These attributes are called principal components. The first principal component accounts for as much of the variability in the data as possible, and each succeeding component accounts for as much of the remaining variability as possible. The model uses sklearn.PCA() which decomposes the dataset into a given number of dimensions.
	
	2)Convert dataset to numpy array data structure for fast and easy calculations: the model stores the data in a pandas dataframe.However,for simple and efficient calculations numpy array data structure is used.Fortunately, the pandas dataframe and numpy array can be type casted into one other.Pandas Dataframe is converted into numpy array simply by using Dataframe.arr(). 
	
	3)Feature standardization for fast processing(StandardScaler): The model standardizes the  features of the dataset by implementing the StandardScalar.fit_transform.This function changes the data in the dataset into a normalized form by removing the mean and scaling to unit variance.This means,
            		x'=(x-mean)/standard-deviation
Centering and scaling is implemented independently on each column of the dataset by computing the relevant statistics on the samples, which are the average and standard deviation value for each column in the training set. Mean and standard deviation are then stored to be used on later data using the transform method.
Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual feature do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).
For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.
	4)Finding cluster labels for each individual row and centroids for each cluster in the dataset : The model now implements K means clustering to the transformed dataset by using sklearn.clusters.KMeans. The model first prepares a model of KMeans with n number of clusters and one random state. Then the columns with numeric value are selected from the transformed dataset. The selected columns are input for the K means clustering and is fit into the prepared K means model by using kmeans.fit().The output for this function returns an array of cluster labels for each point in the dataset. The cluster label array is fetched by using the function kmeans.cluster_labels. The K means algorithm also returns the center points for each cluster.This is fetched by using kmeans._centers_ function from the k means model. 

	5)Reduce the data in two dimensions to plot the graph : The model uses PCA to convert the graph into two dimensions to be plotted using matploitlib.plot() and matploitlib.show() to show the graph.

E) Calculating Silhoulette score for each model
	
Silhouette score analysis can be implemented on the clusters to analyse the separation distance between the clusters of the dataset. The silhouette score plot gives an analysis how distant or close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters and their seperation visually. The value of silhoullette score ranges from -1 to 1.
These values are also referred to as Silhoullete Coefficients.Silhoullete Coefficients near +1 indicate that the point is appropriately placed in the cluster,which means it is close to the points in the same cluster and far enough from the points in neighboring clusters. A value of 0 indicates that the point is probable to be suited in more than one cluster,which means it can probably give wrong inaccurate predictions and negative values indicate that the point is misplaced in the wrong cluster,which means that the predictions for this event will be inaccurate.
The model implements exhaustive searching to find suitable number of clusters for which the number of points having silhoullete score values nearer to +1 are sufficient enough so that the rest of the points can be dropped.The model implements this technique using the following steps:

	1)Finding Distance between centers of clusters: For each center of the cluster obtained by K-means clustering over the dataset,the model finds out the distance between centers every other cluster in the dataset.The distance is calculated using the Euclidean metrics, and 		distance between two points can be given as:
             D(A,B)=((a1-b1)^2+(a2-b2)^2....(an-bn)^2)^0.5
	where n is the total number of dimensions of the point,which for this case is seven. The centers are already stoerd in a numpy array data structure, so the model implements scipy.spatial.distance.cdist(A,B,metric) function imported from the scipy library in anaconda, which takes three inputs as two two dimensional arrays with same number of columns and a metric to calculate the distance. The function returns a two dimensional numpy array with |A| rows and |B| columns and each element (i,j) in the array represents the distance between points i and j. The input to this function in the model is the numpy array of centers and the metric is euclidean. The function returns the distance between the centers of the clusters in a two dimensional matrix.   
              
	2)Mapping clusters with their nearest clusters: Once the distance matrix is calculated, the time complexity for which is O(n^2), the nearest center for each cluster center is mapped into an array whose ith index gives value of the cluster nearest to the index.This process is done by finding the minimum non zero element in each row,which is similar as finding the second minimum of n values and is done in O(log(n)) complexity using Heap Sort.

	3)Store the silhouellete score for each row: The model traverses each row of the dataset and for each row the following steps are implemented to calculate the silhoullete score for the current cluster model:

		a)Get all the rows of the same cluster: In the updated dataframe, where each point has its associated cluster value in the 'cluster' label, to get all the points belonging to the same cluster, DataFrame.group_by(label) is used. The function takes one input as the column on the basis of which the DataFrame has to be grouped.The function returns the re-arranged dataframe grouped in accordance to the parameter provided. The model provides 'cluster' as input in the function, the function returns a dataframe grouped according to the cluster labels for the current cluster model. The model then uses DataFrame.get_group(label) function,which takes as input the atrribute value on basis of which rows are to be extracted from a grouped dataframe.The function returns a  reduced dataframe of rows which contain the attribute value. The model supplies the cluster label of the point as input to this function and hence the output returned by the function is the dataframe of points in the same cluster as the test point.  
		
		b)Get all the rows of the nearest cluster: To get all the rows of the nearest cluster for the test point from the updated dataframe, where each point has its associated cluster value in the 'cluster' label, DataFrame.group_by(label) is used again to group the points according to clusters.The model then uses DataFrame.get_group(label) function,which takes as input the atrribute value on basis of which rows are to be extracted from a grouped dataframe.The function returns a  reduced dataframe of rows which contain the attribute value. The input provided by the model to this function is the value for the nearest cluster for the test point. This value is mapped in the ith index of the nearest cluster map where i is the cluster label of the test point. The model supplies the nearest cluster label of the point as input to this function and hence the output returned by the function is the dataframe of points in the nearest cluster of the test point. 
 
		c)Distance between the point and all the points in the same cluster: Once all the rows with the same cluster label are grouped, the distance between the point and all the points in that group is calculated by using scipy.spatial.distance.cdist(A,B,metric) function imported from the scipy library in anaconda, which takes three inputs as two two dimensional arrays with same number of columns and a metric to calculate the distance. The function returns a two dimensional numpy array with |A| rows and |B| columns and each element (i,j) in the array represents the distance between points i and j.The grouped points are type-casted into a numpy array and then the input supplied to this function in the model is the numpy array of the point,numpy array of all the points in the same cluster  and the metric is euclidean. The function returns the distance between the point and all the other points of the same cluster in a two dimensional matrix. 
  
		d)Distance between the point and all the points in the nearest cluster: Once all the rows with the nearest cluster label are grouped, the distance between the point and all the points in that group is calculated by using scipy.spatial.distance.cdist(A,B,metric) function. The function returns a two dimensional numpy array with |A| rows and |B| columns and each element (i,j) in the array represents the distance between points i and j.The grouped points are type-casted into a numpy array and then the input supplied to this function in the model is the numpy array of the point,numpy array of all the points in the nearest cluster  and the metric is euclidean. The function returns the distance between the point and all the other points of the same nearest in a two dimensional matrix.
		e)Get silhoullete score for each row: The silhoullete score for each point in the dataset in calculated as the ratio of the absolute difference of the mean of distance between all the points in the same cluster and all the points in the nearest cluster and the maximum between the two. That means, if A is the mean of distance between the point and all points in the same cluster as of that point and B is defined as the mean of distance between the point and all points in the cluster nearest to that point, the Silhoullete score is calculated as:
		S=|A-B|/max(A,B)
So, the model calculates the mean of distance between the point and all points in the same cluster as that point and mean of distance between the point and all points in the cluster nearest to that point using numpy.mean(A) function which takes as input a numpy array and returns the mean of all values in the array.The model calls this fuction with inputs same cluster distances and nearest cluster distances for that point. Once these are calculated, the silhoullete score is calculated using the formula.
     

F)Plotting the silhoullete score graph: 

The model uses matploitlib library from anaconda to plot the silhoullette score graph. On the X axis the silhoullete score are plotted and on the Y axis the cluster labels are plotted. The cluster labeles are demarcated by inserting blank space between silhouette plots of individual clusters. From observing the graphs one can chose the perfect model of the dataset with minimal size differences amongst the clusters and minimal number of points with silhoullete score values distant from -1.

SCREENSHOTS OF OUTPUT: 
